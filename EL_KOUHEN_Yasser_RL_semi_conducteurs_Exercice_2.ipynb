{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBGWIJJ6tsh3"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercice 2 : Reinforcement learning dans une grille 5×5**"
      ],
      "metadata": {
        "id": "9nBtTuUxtwwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description**\n",
        "Un agent se déplace dans une grille de 5×5 cases en partant de la position (0,0) et doit atteindre un objectif situé à (4,4). À chaque étape, l'agent peut choisir l'une des quatre actions possibles :\n",
        "\n",
        "\n",
        "1.  Haut (U).\n",
        "2.  Bas (D).\n",
        "3.  Gauche (L).\n",
        "4.  Droite (R)\n",
        "\n",
        "L'agent reçoit une récompense de +1 lorsqu'il atteint la cible et une récompense de -1 à chaque autre étape. S'il tente de sortir des limites de la grille, il reste sur place. L'objectif est d'entraîner un agent utilisant l'algorithme REINFORCE pour apprendre la meilleure politique de déplacement vers la cible en minimisant le nombre d'étapes.\n",
        "\n",
        "# **Questions**\n",
        "\n",
        "### **Compréhension de l’environnement**\n",
        "1.   Quels sont les états possibles ($s_{t}$) de l’agent dans cet environnement ?\n",
        "2.   Quelles sont les actions ($a_{t}$) que l’agent peut effectuer et comment influencent-elles son état ?\n",
        "3.   Comment la fonction de récompense ($R_{t}$) est-elle définie et quel est son impact sur l’apprentissage ?\n",
        "\n",
        "### **Implémentation avec REINFORCE**\n",
        "1.   Comment représenter l’agent sous forme d’un réseau de neurones pour approximer la politique ?\n",
        "2.   Quel est le rôle de la politique stochastique dans l’algorithme REINFORCE\n",
        "3.   Comment la mise à jour des poids du réseau de neurones est-elle effectuée à partir des épisodes joués ?\n",
        "\n",
        "### **Expérimentation et analyse**\n",
        "1.   Après l'entraînement, comment pouvez-vous évaluer si l'agent a bien appris à atteindre la cible efficacement ?\n",
        "2.   Que se passe-t-il si la grille devient plus grande (par exemple, 10×10) ? Quels ajustements seraient nécessaires dans l’apprentissage ?"
      ],
      "metadata": {
        "id": "mMkPP6batz4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compréhension de l'environnement\n",
        "\n",
        "## 1. États possibles\n",
        "Les états possibles ($s_t$) de l'agent sont toutes les positions qu'il peut occuper dans la grille 5×5. Chaque état est une paire de coordonnées (x, y) où x et y sont des entiers entre 0 et 4 inclus. Il y a donc 25 états possibles au total, allant de (0,0) à (4,4).\n",
        "\n",
        "## 2. Actions possibles\n",
        "L'agent peut effectuer quatre actions ($a_t$) :\n",
        "- Haut (U) : déplace l'agent d'une case vers le haut (y+1)\n",
        "- Bas (D) : déplace l'agent d'une case vers le bas (y-1)\n",
        "- Gauche (L) : déplace l'agent d'une case vers la gauche (x-1)\n",
        "- Droite (R) : déplace l'agent d'une case vers la droite (x+1)\n",
        "\n",
        "Ces actions modifient l'état de l'agent en changeant ses coordonnées. Si l'action amène l'agent à sortir des limites de la grille (x < 0, y < 0, x > 4 ou y > 4), l'agent reste dans son état actuel.\n",
        "\n",
        "## 3. Fonction de récompense\n",
        "La fonction de récompense ($R_t$) est définie comme suit :\n",
        "- +1 si l'agent atteint la cible (4,4)\n",
        "- -1 pour chaque autre déplacement\n",
        "\n",
        "Cette fonction de récompense encourage l'agent à atteindre la cible le plus rapidement possible, car chaque action qui ne mène pas directement à la cible est pénalisée. L'impact sur l'apprentissage est que l'agent va chercher à maximiser sa récompense cumulée en trouvant le chemin le plus court vers la cible.\n",
        "\n"
      ],
      "metadata": {
        "id": "6zH5OpjeAcOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implémentation avec REINFORCE\n",
        "\n",
        "## 1. Représentation de l'agent sous forme de réseau de neurones\n",
        "\n",
        "Dans cette implémentation, l'agent est représenté par un réseau de neurones qui approxime la politique. La structure est la suivante :\n",
        "- Une couche d'entrée de dimension 2 (correspondant aux coordonnées x et y de l'agent)\n",
        "- Une couche cachée de dimension 128 avec activation ReLU\n",
        "- Une couche de sortie de dimension 4 (correspondant aux 4 actions possibles) avec activation softmax\n",
        "\n",
        "La sortie du réseau est une distribution de probabilité sur les actions possibles, ce qui permet à l'agent de prendre des décisions stochastiques.\n",
        "\n",
        "## 2. Rôle de la politique stochastique dans REINFORCE\n",
        "\n",
        "La politique stochastique joue un rôle crucial dans l'algorithme REINFORCE pour plusieurs raisons :\n",
        "\n",
        "1. **Exploration de l'environnement** : En choisissant les actions avec une certaine probabilité plutôt que de manière déterministe, l'agent explore différentes trajectoires dans l'environnement, ce qui lui permet de découvrir de meilleures stratégies.\n",
        "\n",
        "2. **Échantillonnage des trajectoires** : REINFORCE est un algorithme de type Monte-Carlo qui nécessite d'échantillonner des trajectoires complètes. La stochasticité assure une diversité de trajectoires pour l'apprentissage.\n",
        "\n",
        "3. **Calcul du gradient de la politique** : Dans REINFORCE, le gradient de la politique est estimé à partir des trajectoires échantillonnées. La formule mathématique du gradient implique les probabilités des actions, ce qui nécessite une politique stochastique.\n",
        "\n",
        "Dans l'implémentation, la politique stochastique est obtenue en appliquant une fonction softmax à la sortie du réseau de neurones, ce qui donne une distribution de probabilité sur les actions. L'agent sélectionne ensuite une action en échantillonnant à partir de cette distribution.\n",
        "\n",
        "## 3. Mise à jour des poids du réseau\n",
        "\n",
        "La mise à jour des poids du réseau dans REINFORCE se fait selon les étapes suivantes :\n",
        "\n",
        "1. Collecter une trajectoire complète (un épisode) en suivant la politique actuelle\n",
        "2. Pour chaque étape t de la trajectoire, calculer le rendement G_t (somme des récompenses futures escomptées)\n",
        "3. Mettre à jour les poids du réseau en utilisant le gradient de la politique pondéré par le rendement\n",
        "\n",
        "Mathématiquement, la règle de mise à jour est :\n",
        "θ = θ + α ∇_θ log π_θ(a_t|s_t) G_t\n",
        "\n",
        "Où :\n",
        "- θ représente les paramètres du réseau\n",
        "- α est le taux d'apprentissage\n",
        "- ∇_θ log π_θ(a_t|s_t) est le gradient du logarithme de la probabilité de l'action a_t dans l'état s_t\n",
        "- G_t est le rendement à partir de l'étape t\n",
        "\n",
        "Dans l'implémentation, cette mise à jour est effectuée dans la méthode `update_policy()` de la classe `ReinforceAgent`. Les rendements sont calculés, puis normalisés pour réduire la variance. Ensuite, la perte de la politique est calculée comme la somme des produits négatifs des logarithmes des probabilités des actions et des rendements correspondants. Enfin, la rétropropagation est utilisée pour mettre à jour les poids du réseau.\n",
        "\n"
      ],
      "metadata": {
        "id": "dXvvr0J-ArMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Définition de l'environnement\n",
        "class GridEnvironment:\n",
        "    def __init__(self, grid_size=5):\n",
        "        self.grid_size = grid_size\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = (0, 0)  # Position initiale\n",
        "        self.target = (self.grid_size-1, self.grid_size-1)  # Position cible\n",
        "        self.done = False\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Convertir la position en un état unique\n",
        "        x, y = self.position\n",
        "        return np.array([x, y])\n",
        "\n",
        "    def step(self, action):\n",
        "        # Définition des déplacements\n",
        "        moves = {\n",
        "            0: (0, 1),   # Haut (U)\n",
        "            1: (0, -1),  # Bas (D)\n",
        "            2: (-1, 0),  # Gauche (L)\n",
        "            3: (1, 0)    # Droite (R)\n",
        "        }\n",
        "\n",
        "        x, y = self.position\n",
        "        dx, dy = moves[action]\n",
        "        new_x, new_y = x + dx, y + dy\n",
        "\n",
        "        # Vérifier si la nouvelle position est dans les limites\n",
        "        if 0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size:\n",
        "            self.position = (new_x, new_y)\n",
        "\n",
        "        # Vérifier si l'agent a atteint la cible\n",
        "        self.done = self.position == self.target\n",
        "\n",
        "        # Définir la récompense\n",
        "        if self.done:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = -1.0\n",
        "\n",
        "        return self._get_state(), reward, self.done\n",
        "\n",
        "# Réseau de neurones pour la politique\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "# Agent REINFORCE\n",
        "class ReinforceAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self):\n",
        "        R = 0\n",
        "        policy_loss = []\n",
        "        returns = []\n",
        "\n",
        "        # Calculer les rendements (retours) pour chaque pas de temps\n",
        "        for r in self.rewards[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        returns = torch.tensor(returns)\n",
        "\n",
        "        # Normaliser les rendements pour réduire la variance\n",
        "        if len(returns) > 1:\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "        # Calculer la perte de la politique\n",
        "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * R)\n",
        "\n",
        "        # Mettre à jour les poids du réseau\n",
        "        if policy_loss:  # Vérifier que la liste n'est pas vide\n",
        "            self.optimizer.zero_grad()\n",
        "            # Utiliser stack au lieu de cat pour les tenseurs de dimension 0\n",
        "            policy_loss = torch.stack(policy_loss).sum()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Réinitialiser les mémoires\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "# Fonction d'entraînement\n",
        "def train(env, agent, max_episodes=1000, max_steps=100):\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            agent.rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.update_policy()\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Épisode {episode}, Récompense moyenne: {np.mean(episode_rewards[-100:])}\")\n",
        "\n",
        "    return episode_rewards\n",
        "\n",
        "# Fonction d'évaluation\n",
        "def evaluate(env, agent, num_episodes=10):\n",
        "    total_steps = 0\n",
        "    success = 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            # Utiliser la politique apprise de manière déterministe (prendre l'action la plus probable)\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            with torch.no_grad():\n",
        "                probs = agent.policy(state_tensor)\n",
        "            action = probs.argmax().item()\n",
        "\n",
        "            next_state, _, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        if done:\n",
        "            success += 1\n",
        "            total_steps += steps\n",
        "\n",
        "    avg_steps = total_steps / success if success > 0 else float('inf')\n",
        "    success_rate = success / num_episodes\n",
        "\n",
        "    return avg_steps, success_rate\n",
        "\n",
        "# Programme principal\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridEnvironment(grid_size=5)\n",
        "    state_dim = 2  # x, y\n",
        "    action_dim = 4  # U, D, L, R\n",
        "\n",
        "    agent = ReinforceAgent(state_dim, action_dim, lr=0.01, gamma=0.99)\n",
        "\n",
        "    # Entraînement\n",
        "    rewards = train(env, agent, max_episodes=2000, max_steps=100)\n",
        "\n",
        "    # Évaluation\n",
        "    avg_steps, success_rate = evaluate(env, agent, num_episodes=100)\n",
        "    print(f\"Taux de réussite: {success_rate:.2f}\")\n",
        "    print(f\"Nombre moyen d'étapes pour atteindre la cible: {avg_steps:.2f}\")\n",
        "\n",
        "    # Visualiser la politique apprise\n",
        "    grid_size = 5\n",
        "    policy_grid = np.zeros((grid_size, grid_size), dtype=int)\n",
        "\n",
        "    for y in range(grid_size):\n",
        "        for x in range(grid_size):\n",
        "            state = np.array([x, y])\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            with torch.no_grad():\n",
        "                probs = agent.policy(state_tensor)\n",
        "            action = probs.argmax().item()\n",
        "            policy_grid[grid_size-1-y, x] = action\n",
        "\n",
        "    # Afficher la politique\n",
        "    action_symbols = ['↑', '↓', '←', '→']\n",
        "    for row in policy_grid:\n",
        "        print(' '.join([action_symbols[a] for a in row]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3leiJRJA-fD",
        "outputId": "aa078945-24ec-4876-a258-ad7304dcdc82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Épisode 0, Récompense moyenne: -100.0\n",
            "Épisode 100, Récompense moyenne: -38.57\n",
            "Épisode 200, Récompense moyenne: -6.52\n",
            "Épisode 300, Récompense moyenne: -6.76\n",
            "Épisode 400, Récompense moyenne: -7.02\n",
            "Épisode 500, Récompense moyenne: -6.23\n",
            "Épisode 600, Récompense moyenne: -6.06\n",
            "Épisode 700, Récompense moyenne: -6.14\n",
            "Épisode 800, Récompense moyenne: -6.05\n",
            "Épisode 900, Récompense moyenne: -6.47\n",
            "Épisode 1000, Récompense moyenne: -6.07\n",
            "Épisode 1100, Récompense moyenne: -6.0\n",
            "Épisode 1200, Récompense moyenne: -6.52\n",
            "Épisode 1300, Récompense moyenne: -6.03\n",
            "Épisode 1400, Récompense moyenne: -6.05\n",
            "Épisode 1500, Récompense moyenne: -6.18\n",
            "Épisode 1600, Récompense moyenne: -6.02\n",
            "Épisode 1700, Récompense moyenne: -6.02\n",
            "Épisode 1800, Récompense moyenne: -6.02\n",
            "Épisode 1900, Récompense moyenne: -6.06\n",
            "Taux de réussite: 1.00\n",
            "Nombre moyen d'étapes pour atteindre la cible: 8.00\n",
            "→ → → → ↑\n",
            "→ → → ↑ ↑\n",
            "→ → ↑ ↑ ↑\n",
            "→ → ↑ ↑ ↑\n",
            "→ ↑ ↑ ↑ ↑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expérimentation et analyse\n",
        "\n",
        "## 1. Évaluation de l'apprentissage\n",
        "\n",
        "Pour évaluer si l'agent a bien appris à atteindre la cible efficacement, on peut utiliser plusieurs métriques :\n",
        "\n",
        "1. **Taux de réussite** : Proportion d'épisodes où l'agent atteint la cible dans une limite de temps/étapes donnée.\n",
        "2. **Nombre moyen d'étapes** : Nombre moyen d'étapes nécessaires pour atteindre la cible. Dans notre cas, le chemin optimal nécessite 8 étapes (4 pas vers la droite et 4 pas vers le haut), donc un agent efficace devrait s'approcher de cette valeur.\n",
        "3. **Récompense cumulée** : La somme des récompenses obtenues pendant un épisode. Pour le chemin optimal, cette valeur serait de -7+1 = -6 (7 étapes à -1 et l'arrivée à +1).\n",
        "4. **Visualisation de la politique** : On peut représenter graphiquement la politique apprise en affichant l'action la plus probable pour chaque état.\n",
        "\n",
        "Dans l'implémentation, la fonction `evaluate()` calcule le taux de réussite et le nombre moyen d'étapes, tandis que la dernière partie du code visualise la politique apprise sous forme de flèches dans une grille.\n",
        "\n",
        "Un agent qui a bien appris aura un taux de réussite proche de 100%, un nombre moyen d'étapes proche de 8, et une politique qui montre clairement un chemin direct vers la cible.\n",
        "\n",
        "## 2. Adaptation à une grille plus grande\n",
        "\n",
        "Si la grille devient plus grande (par exemple, 10×10), plusieurs ajustements seraient nécessaires :\n",
        "\n",
        "1. **Architecture du réseau** : Une grille plus grande peut nécessiter un réseau plus complexe, avec plus de neurones dans la couche cachée ou des couches supplémentaires pour capturer la complexité accrue de l'environnement.\n",
        "\n",
        "2. **Paramètres d'apprentissage** :\n",
        "   - Augmenter le nombre d'épisodes d'entraînement pour permettre une exploration plus complète\n",
        "   - Ajuster le taux d'apprentissage, potentiellement le réduire pour une convergence plus stable\n",
        "   - Modifier le facteur d'escompte γ pour équilibrer les récompenses à court et à long terme\n",
        "\n",
        "3. **Exploration vs exploitation** : Dans un espace d'états plus grand, l'exploration devient plus critique. On pourrait :\n",
        "   - Ajouter un mécanisme d'exploration explicite (comme epsilon-greedy)\n",
        "   - Implémenter une planification de la température pour le softmax\n",
        "   - Utiliser des techniques d'initialisation des poids qui favorisent l'exploration initiale\n",
        "\n",
        "4. **Réduction de la variance** : Dans des environnements plus grands, la variance des estimations de gradient peut augmenter, ce qui ralentit l'apprentissage. Des techniques comme :\n",
        "   - L'ajout d'une référence (baseline) pour réduire la variance\n",
        "   - L'utilisation d'avantages au lieu de rendements bruts\n",
        "   - L'implémentation d'algorithmes plus avancés comme A2C ou PPO\n",
        "\n",
        "5. **Représentation des états** : Pour une grande grille, une représentation plus efficace des états pourrait être bénéfique, comme l'utilisation de la distance relative à la cible plutôt que des coordonnées absolues.\n",
        "\n",
        "En pratique, pour une grille 10×10, on pourrait commencer par doubler le nombre d'épisodes d'entraînement, augmenter la taille de la couche cachée à 256 ou 512 neurones, et réduire légèrement le taux d'apprentissage pour assurer une convergence plus stable."
      ],
      "metadata": {
        "id": "vhqJ6cTtAxV2"
      }
    }
  ]
}